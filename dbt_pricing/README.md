# MTG Pricing - dbt Project

## Overview

This dbt project transforms MTG card pricing data from MongoDB (unpivoted format) into Delta Lake (pivoted, partitioned format) for fast analytics queries.

**Performance Gains:**
- ðŸš€ 97% storage reduction (9.8M rows â†’ 78k rows/day)
- âš¡ 100-1000x faster queries (partition pruning + compression)
- ðŸ’¾ 70-80% compression (Parquet + Snappy)
- ðŸ“Š Optimized for analytics (Z-ordered by card_name)

---

## Architecture

```
Scryfall API                MongoDB (Cosmos DB)          dbt Transformation           Delta Lake (ADLS Gen2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bulk Pricing â”‚           â”‚ card_pricing_   â”‚         â”‚ stg_pricing_raw  â”‚        â”‚ pricing_daily    â”‚
â”‚ Collection   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ daily           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ (view)           â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ (partitioned)    â”‚
â”‚              â”‚           â”‚                 â”‚         â”‚                  â”‚        â”‚                  â”‚
â”‚ 78k cards    â”‚           â”‚ 9.8M rows       â”‚         â”‚ int_pricing_     â”‚        â”‚ 78k rows/day     â”‚
â”‚ (batches)    â”‚           â”‚ 8.9GB           â”‚         â”‚ pivoted          â”‚        â”‚ 45MB/day         â”‚
â”‚              â”‚           â”‚ (unpivoted)     â”‚         â”‚ (pivot)          â”‚        â”‚ (compressed)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                           â”‚                                                          â”‚
      â”‚                           â”‚                                                          â–¼
      â”‚                           â–¼                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                    Keep 7 days only                                         â”‚ pricing_latest   â”‚
      â”‚                    (auto-cleanup)                                           â”‚ (view)           â”‚
      â”‚                                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                                                                      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          Databricks Scheduled Jobs (Daily 1:00 AM UTC)
```

**Data Flow**:
1. **Ingestion** (`databricks_notebooks/01_ingest_scryfall_pricing.py`): Scryfall API â†’ MongoDB
2. **Transformation** (`databricks_notebooks/02_dbt_transformation.py`): MongoDB â†’ Delta Lake via dbt
3. **Query** (Web app): Delta Lake REST API

---

## Setup Instructions

### 1. Install dbt

```bash
# Install dbt-databricks adapter
pip install dbt-databricks
```

### 2. Configure Databricks Connection

Add to `.env`:
```bash
DATABRICKS_HOST=adb-1234567890123456.7.azuredatabricks.net
DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/abc123def456
DATABRICKS_TOKEN=dapi***
```

Get credentials from Azure Databricks:
1. Open your Databricks workspace
2. Click **SQL Warehouses** â†’ Select your warehouse
3. Copy **Server hostname** (DATABRICKS_HOST)
4. Copy **HTTP path** (DATABRICKS_HTTP_PATH)
5. Click **User Settings** â†’ **Access Tokens** â†’ Generate token

### 3. Test Connection

```bash
cd dbt_pricing
dbt debug
```

Expected output:
```
Connection:
  host: adb-***.azuredatabricks.net
  http_path: /sql/1.0/warehouses/***
  catalog: mtgecorec_dev
  schema: pricing
  Connection test: [OK connection ok]
```

### 4. Run Models

```bash
# Run all models
dbt run

# Run specific model
dbt run --select stg_pricing_raw

# Run with full refresh
dbt run --full-refresh

# Test data quality
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

---

## Project Structure

```
dbt_pricing/
â”œâ”€â”€ dbt_project.yml           # Project configuration
â”œâ”€â”€ profiles.yml              # Databricks connection
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_pricing_raw.sql        # Load from MongoDB
â”‚   â”‚   â””â”€â”€ sources.yml                # Source definitions
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ int_pricing_pivoted.sql    # Pivot transformation
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ pricing_daily.sql          # Final partitioned table
â”‚       â”œâ”€â”€ pricing_latest.sql         # Latest prices view
â”‚       â””â”€â”€ schema.yml                 # Model documentation
â”œâ”€â”€ macros/                   # SQL macros (reusable code)
â”œâ”€â”€ tests/                    # Data quality tests
â””â”€â”€ target/                   # Compiled SQL (gitignored)
```

---

## Data Models

### Staging Layer

**stg_pricing_raw**
- Materialized as: View
- Source: MongoDB `card_pricing_daily` collection
- Purpose: Load raw data with minimal transformations
- Schema: Unpivoted (6 rows per card per day)

### Intermediate Layer

**int_pricing_pivoted**
- Materialized as: View
- Purpose: Transform from long to wide format
- Transformation: 6 rows â†’ 1 row per card
- Adds: Calculated fields (cheapest price, foil premium)

### Marts Layer

**pricing_daily**
- Materialized as: Incremental (Delta Lake)
- Partition: By `ds` (date)
- File format: Delta (Parquet + transaction log)
- Optimization: Z-ordered by `card_name`
- Purpose: Production table for analytics queries

**pricing_latest**
- Materialized as: View
- Purpose: Most recent pricing data
- Use case: Web application queries

---

## Common Queries

### Get Latest Prices
```sql
SELECT card_name, usd, usd_foil 
FROM pricing_latest
WHERE card_name LIKE 'Mana Crypt%'
```

### Price History
```sql
SELECT ds, card_name, usd
FROM pricing_daily
WHERE card_name = 'Sol Ring'
AND ds >= CURRENT_DATE() - INTERVAL 30 DAYS
ORDER BY ds DESC
```

### Find Expensive Cards
```sql
SELECT card_name, usd, usd_foil
FROM pricing_latest
WHERE usd > 100
ORDER BY usd DESC
LIMIT 20
```

### Foil Premium Analysis
```sql
SELECT 
  card_name,
  usd,
  usd_foil,
  foil_premium_pct
FROM pricing_latest
WHERE foil_premium_pct > 100
ORDER BY foil_premium_pct DESC
```

---

## Development Workflow

### Local Development
1. Make changes to SQL models
2. Test locally: `dbt run --target dev`
3. Validate: `dbt test`
4. Review compiled SQL: `target/compiled/`

### Deploy to Production
```bash
# Deploy to production catalog
dbt run --target prod

# Or use Databricks workflow (scheduled daily)
# See: databricks_notebooks/02_dbt_transformation.py
```

---

## Performance Tuning

### Optimize Delta Tables
```sql
-- Run after large updates
OPTIMIZE pricing_daily ZORDER BY (card_name);

-- Clean up old files (keep 7 days)
VACUUM pricing_daily RETAIN 168 HOURS;
```

### Incremental Strategy
- Default: Merge (upsert based on `scryfall_id`)
- Full refresh: `dbt run --full-refresh`
- Process new dates only: `WHERE ds > MAX(ds)`

---

## Data Quality Tests

Built-in tests:
- âœ… Unique scryfall_id per date
- âœ… Non-null prices
- âœ… Positive price values
- âœ… Valid price types
- âœ… Date partition integrity

Run tests:
```bash
dbt test                    # All tests
dbt test --select pricing_daily  # Specific model
```

---

## Troubleshooting

### Connection Issues
```bash
# Check credentials
dbt debug

# Verify .env variables loaded
echo $DATABRICKS_HOST
```

### Performance Issues
```bash
# Check table statistics
DESCRIBE DETAIL pricing_daily;

# Check partition sizes
SELECT ds, COUNT(*) 
FROM pricing_daily 
GROUP BY ds 
ORDER BY ds DESC;
```

### Data Issues
```bash
# Validate source data
dbt run --select stg_pricing_raw

# Check row counts
dbt test --select source:mongodb
```

---

## Next Steps

After setup:
1. âœ… Upload notebooks to Databricks (see `databricks_notebooks/README.md`)
2. âœ… Create secret scope with credentials
3. âœ… Schedule daily pipeline (1:00 AM ingestion, 1:30 AM transformation)
4. âœ… Run initial backfill (30 days)
5. âœ… Update web app to query Delta Lake
6. âœ… Archive MongoDB pricing (keep 7 days)
7. âœ… Monitor query performance
8. âœ… Disable Azure Functions pipeline

---

## Cost Optimization

- **Databricks SQL Warehouse**: Serverless, pay-per-query (~$0.22/DBU)
- **Storage**: ADLS Gen2 (~$0.02/GB/month)
- **Compute**: Job clusters (auto-terminate after run)

**Estimated monthly cost:** $15-25 (vs $80-140 current)
